@article{mocanu_scalable_2018,
	title = {Scalable {Training} of {Artificial} {Neural} {Networks} with {Adaptive} {Sparse} {Connectivity} inspired by {Network} {Science}},
	volume = {9},
	url = {http://arxiv.org/abs/1707.04780},
	urldate = {2020-02-26},
	journal = {Nature Communications},
	author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
	year = {2018},
}

@misc{klear_sparse_2018,
	title = {The {Sparse} {Future} of {Deep} {Learning}},
	url = {https://towardsdatascience.com/the-sparse-future-of-deep-learning-bce05e8e094a},
	abstract = {Could this be the next quantum leap in AI research?},
	language = {en},
	urldate = {2020-02-26},
	journal = {Medium},
	author = {Klear, Michael},
	month = dec,
	year = {2018},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/Users/Russel/Zotero/storage/PUA8NW8E/the-sparse-future-of-deep-learning-bce05e8e094a.html:text/html}
}

@misc{sparse_vs_dense,
  author = {Amir Alavi},
  title = {{Using a VNN architecture}},
  howpublished = "\url{https://amiralavi.net/blog/2018/07/29/vnn-implementation}",
  year = {2018}, 
  note = "[Online; accessed 26/02/2020]"
}

@misc{cnn_eli5,
    author = {Sumit Saha},
    title = {{A comprehensive guide to convolutional neural networks: The ELI5 way}},
    howpublished = "\url{https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53}",
    year = {2018},
    note = "[Online; accessed 26/02/2020]"
}

@misc{numenta_sparse_pruning,
    author = {Lucas Souza},
    title = {{A case for sparsity in neural networks: Pruning}},
    howpublished = "\url{https://numenta.com/blog/2019/08/30/case-for-sparsity-in-neural-networks-part-1-pruning}",
    year = {2019},
    note = "[Online; accessed 26/02/2020]"
}

@misc{sparse_brain_numenta,
    author = {numenta.com},
    title = {{Sparse distributed representations}},
    howpublished = "\url{https://numenta.com/neuroscience-research/sparse-distributed-representations/}",
    year = {2018},
    note = "[Online; accessed 26/02/2020]"
}

@misc{sparse_social_network,
    author = {Sean Barnes},
    title = {{Researchers Propose Social Network Modeling to Fight Hospital Infections}},
    howpublished = "\url{https://umdrightnow.umd.edu/news/researchers-propose-social-network-modeling-fight-hospital-infections}",
    year = {2013},
    note = "[Online; accessed 26/02/2020]"
}

@article{ahmad2016neurons,
  title={How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites},
  author={Ahmad, Subutai and Hawkins, Jeff},
  journal={arXiv preprint arXiv:1601.00720},
  year={2016}
}

@article{cui2016continuous,
  title={Continuous online sequence learning with an unsupervised neural network model},
  author={Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  journal={Neural computation},
  volume={28},
  number={11},
  pages={2474--2504},
  year={2016},
  publisher={MIT Press}
}

@article{golkar2019continual,
  title={Continual learning via neural pruning},
  author={Golkar, Siavash and Kagan, Michael and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1903.04476},
  year={2019}
}

@article{fukushima1988neocognitron,
  title={Neocognitron: A hierarchical neural network capable of visual pattern recognition},
  author={Fukushima, Kunihiko},
  journal={Neural networks},
  volume={1},
  number={2},
  pages={119--130},
  year={1988},
  publisher={Elsevier}
}


@article{ma2018using,
  title={Using deep learning to model the hierarchical structure and function of a cell},
  author={Ma, Jianzhu and Yu, Michael Ku and Fong, Samson and Ono, Keiichiro and Sage, Eric and Demchak, Barry and Sharan, Roded and Ideker, Trey},
  journal={Nature methods},
  volume={15},
  number={4},
  pages={290},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{changpinyo2017power,
  title={The power of sparsity in convolutional neural networks},
  author={Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
  journal={arXiv preprint arXiv:1702.06257},
  year={2017}
}
